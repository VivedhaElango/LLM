{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune classifier with ModernBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation example we will understand how to fine-tune ModernBERT, for classifying user prompts to implement an intelligent LLM router.\n",
    "You will learn how to:\n",
    "1. Setup environment and install libraries\n",
    "2. Load and prepare the classification dataset\n",
    "3. Fine-tune & evaluate ModernBERT with the Hugging Face Trainer\n",
    "4. Run inference & test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup environment and install libraries\n",
    "Our first step is to install Hugging Face Libraries and Pyroch, including transformers and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.4.1\n",
      "  Using cached torch-2.4.1-cp310-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from torch==2.4.1) (4.12.2)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>1.9 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from tensorboard) (24.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.69.0-cp310-cp310-macosx_12_0_universal2.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from tensorboard) (1.26.4)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6\n",
      "  Downloading protobuf-5.29.2-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.8/417.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from tensorboard) (65.5.0)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, tensorboard-data-server, sympy, protobuf, networkx, MarkupSafe, markdown, grpcio, fsspec, filelock, absl-py, werkzeug, jinja2, torch, tensorboard\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.1.0 filelock-3.16.1 fsspec-2024.12.0 grpcio-1.69.0 jinja2-3.1.5 markdown-3.7 mpmath-1.3.0 networkx-3.4.2 protobuf-5.29.2 sympy-1.13.3 tensorboard-2.18.0 tensorboard-data-server-0.7.2 torch-2.4.1 werkzeug-3.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.2.post1.tar.gz (3.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/qg/kw58x9bs58j2b41h2tz5579r0000gn/T/pip-install-xx9e02k1/flash-attn_fc096a82e17c41628417ac91946dec38/setup.py\", line 19, in <module>\n",
      "  \u001b[31m   \u001b[0m     from wheel.bdist_wheel import bdist_wheel as _bdist_wheel\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'wheel'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting datasets==3.1.0\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==1.2.1\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hf-transfer==0.1.8\n",
      "  Downloading hf_transfer-0.1.8-cp310-cp310-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from datasets==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from datasets==3.1.0) (4.67.1)\n",
      "Requirement already satisfied: packaging in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from datasets==3.1.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from datasets==3.1.0) (6.0.2)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-macosx_12_0_arm64.whl (29.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.23.0\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Requirement already satisfied: filelock in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from datasets==3.1.0) (3.16.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from datasets==3.1.0) (2.32.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Requirement already satisfied: aiohttp in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from datasets==3.1.0) (3.11.10)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from accelerate==1.2.1) (2.4.1)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.0-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m408.5/408.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from accelerate==1.2.1) (6.0.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (1.18.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (2.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (0.2.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (24.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.1.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.1.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.1.0) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.1.0) (2.2.3)\n",
      "Requirement already satisfied: sympy in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==1.2.1) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==1.2.1) (3.1.5)\n",
      "Requirement already satisfied: networkx in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==1.2.1) (3.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from pandas->datasets==3.1.0) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.1.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==1.2.1) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==1.2.1) (1.3.0)\n",
      "Installing collected packages: pytz, xxhash, tzdata, safetensors, pyarrow, hf-transfer, fsspec, dill, pandas, multiprocess, huggingface-hub, accelerate, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "Successfully installed accelerate-1.2.1 datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 hf-transfer-0.1.8 huggingface-hub-0.27.1 multiprocess-0.70.16 pandas-2.2.3 pyarrow-18.1.0 pytz-2024.2 safetensors-0.5.0 tzdata-2024.2 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/huggingface/transformers.git@6e0515e99c39444caae39472ee1b2fd76ece32f1\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision 6e0515e99c39444caae39472ee1b2fd76ece32f1) to /private/var/folders/qg/kw58x9bs58j2b41h2tz5579r0000gn/T/pip-req-build-yl198iea\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/qg/kw58x9bs58j2b41h2tz5579r0000gn/T/pip-req-build-yl198iea\n",
      "  Running command git rev-parse -q --verify 'sha^6e0515e99c39444caae39472ee1b2fd76ece32f1'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers.git 6e0515e99c39444caae39472ee1b2fd76ece32f1\n",
      "  Running command git checkout -q 6e0515e99c39444caae39472ee1b2fd76ece32f1\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 6e0515e99c39444caae39472ee1b2fd76ece32f1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from transformers==4.48.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from transformers==4.48.0.dev0) (0.5.0)\n",
      "Requirement already satisfied: requests in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from transformers==4.48.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from transformers==4.48.0.dev0) (0.27.1)\n",
      "Requirement already satisfied: filelock in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from transformers==4.48.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from transformers==4.48.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from transformers==4.48.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from transformers==4.48.0.dev0) (6.0.2)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from transformers==4.48.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (2024.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from requests->transformers==4.48.0.dev0) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from requests->transformers==4.48.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from requests->transformers==4.48.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from requests->transformers==4.48.0.dev0) (3.10)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.48.0.dev0-py3-none-any.whl size=10328597 sha256=565cac4d420654e5b973e3f35ba629d125f427c2f5e0a466a8f6c23521e4e6f2\n",
      "  Stored in directory: /Users/krishnapriya/Library/Caches/pip/wheels/64/d9/d6/396433cf9915f92ae21bf08ee24c69de076f8befdefbbf4cb1\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.21.0 transformers-4.48.0.dev0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "!pip install \"torch==2.4.1\" tensorboard \n",
    "!pip install flash-attn \"setuptools<71.0.0\" scikit-learn \n",
    " \n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"datasets==3.1.0\" \\\n",
    "  \"accelerate==1.2.1\" \\\n",
    "  \"hf-transfer==0.1.8\"\n",
    "  #\"transformers==4.47.1\" \\\n",
    " \n",
    "# ModernBERT is not yet available in an official release, so we need to install it from github\n",
    "!pip install \"git+https://github.com/huggingface/transformers.git@6e0515e99c39444caae39472ee1b2fd76ece32f1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp310-cp310-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.15.0-cp310-cp310-macosx_14_0_arm64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-macosx_11_0_arm64.whl (65 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-macosx_11_0_arm64.whl (253 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.3/253.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Downloading pillow-11.1.0-cp310-cp310-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.3-cp310-cp310-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/krishnapriya/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: threadpoolctl, scipy, pyparsing, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.0 pillow-11.1.0 pyparsing-3.2.1 scikit-learn-1.6.0 scipy-1.15.0 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll utilize the Hugging Face Hub as a remote service for model versioning. This allows us to automatically upload our model, logs, and related information to the Hub during training. To get started, you'll need to create an account on Hugging Face. Once registered, we'll use the login utility from the huggingface_hub package to log in and securely save your access token on your local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    " \n",
    "login(token=\"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\") # ADD YOUR TOKEN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and prepare the classification dataset\n",
    "In this example, our goal is to fine-tune ModernBERT to function as a router for user prompts. To achieve this, we require a classification dataset that includes user prompts along with their corresponding \"difficulty\" scores. For this purpose, we'll use the DevQuasar/llm_router_dataset-synth dataset-a synthetic collection of approximately 15,000 user prompts labeled with a difficulty score of either \"large_llm\" (1) or \"small_llm\" (0).\n",
    "To load this dataset, we'll leverage the load_dataset() function from the ğŸ¤— Datasets library, ensuring seamless integration into our fine-tuning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10003/10003 [00:00<00:00, 673239.67 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3080/3080 [00:00<00:00, 878806.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 10003\n",
      "Test dataset size: 3080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Dataset id from huggingface.co/dataset\n",
    "# dataset_id = \"DevQuasar/llm_router_dataset-synth\"\n",
    "dataset_id = \"legacy-datasets/banking77\"\n",
    " \n",
    "# Load raw dataset\n",
    "raw_dataset = load_dataset(dataset_id)\n",
    " \n",
    "print(f\"Train dataset size: {len(raw_dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(raw_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train dataset size: 10003 Test dataset size: 3080\n",
    "Let's check out an example of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I forgot my passcode. Now what?', 'label': 44}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randrange\n",
    " \n",
    "random_id = randrange(len(raw_dataset['train']))\n",
    "raw_dataset['train'][random_id]\n",
    "# {'id': '6225a9cd-5cba-4840-8e21-1f9cf2ded7e6',\n",
    "# 'prompt': 'How many legs does a spider have?',\n",
    "# 'label': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we need to convert our text prompts to token IDs. This is done by a Tokenizer, which tokenizes the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10003/10003 [00:01<00:00, 6934.10 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3080/3080 [00:00<00:00, 7304.08 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    " \n",
    "# Model id to load the tokenizer\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "# model_id = \"google-bert/bert-base-uncased\"\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 512 # set model_max_length to 512 as prompts are not longer than 1024 tokens\n",
    " \n",
    "# Tokenize helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    " \n",
    "# Tokenize dataset\n",
    "raw_dataset =  raw_dataset.rename_column(\"label\", \"labels\") # to match Trainer\n",
    "tokenized_dataset = raw_dataset.map(tokenize, batched=True,remove_columns=[\"text\"])\n",
    " \n",
    "print(tokenized_dataset[\"train\"].features.keys())\n",
    "# dict_keys(['input_ids', 'token_type_ids', 'attention_mask','lable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tune & evaluate ModernBERT with the Hugging FaceÂ Trainer\n",
    "Once our dataset is preprocessed, we're ready to train the model. For this, we'll use the answerdotai/ModernBERT-base model. The first step involves loading the model using the AutoModelForSequenceClassification class from the Hugging Face Hub. This initializes ModernBERT's pre-trained weights and adds a classification head on top. We'll specify the number of classes (2) from our dataset and include the label names to ensure the outputs are interpretable during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    " \n",
    "# Model id to load the tokenizer\n",
    "# model_id = \"answerdotai/ModernBERT-base\"\n",
    "model_id = \"google-bert/bert-base-uncased\"\n",
    " \n",
    "# Prepare model labels - useful for inference\n",
    "labels = tokenized_dataset[\"train\"].features[\"labels\"].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    " \n",
    "# Download the model from huggingface.co/models\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our model during training. The Trainer supports evaluation during training by providing a compute_metrics method. We use the evaluate library to calculate the f1 metric during training on our test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    " \n",
    "# Metric helper method\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    score = f1_score(\n",
    "            labels, predictions, labels=labels, pos_label=1, average=\"weighted\"\n",
    "        )\n",
    "    return {\"f1\": float(score) if score == 1 else score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define the hyperparameters (TrainingArguments) we use for our training. Here we are adding optimizations introduced features for fast training times using torch_compile option in the TrainingArguments.\n",
    "We also leverage the Hugging Face Hub integration of the Trainer to push our checkpoints, logs, and metrics during training into a repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Trainer, TrainingArguments\n",
    " \n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"modernbert-llm-router\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "  num_train_epochs=5,\n",
    "    bf16=True, # bfloat16 training \n",
    "    optim=\"adamw_torch_fused\", # improved optimizer \n",
    "    # logging & evaluation strategies\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_token=HfFolder.get_token(),\n",
    " \n",
    ")\n",
    " \n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocksâ€¦ To disable this warning, you can either:  \n",
    "Avoid using tokenizers before the fork if possible  \n",
    "Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false) huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocksâ€¦ To disable this warning, you can either:  \n",
    "Avoid using tokenizers before the fork if possible  \n",
    "Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)  \n",
    "\n",
    "We can start our training by using the train method of the Trainer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`fused=True` requires all the params to be floating point Tensors of supported devices: ['cuda', 'xpu', 'cpu', 'privateuseone'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/trainer.py:2154\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2152\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2153\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2161\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/trainer.py:2264\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m deepspeed_init(\u001b[38;5;28mself\u001b[39m, num_training_steps\u001b[38;5;241m=\u001b[39mmax_steps)\n\u001b[1;32m   2263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delay_optimizer_creation:\n\u001b[0;32m-> 2264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_training_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m TrainerState(\n\u001b[1;32m   2267\u001b[0m     stateful_callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   2268\u001b[0m         cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cb, ExportableState)\n\u001b[1;32m   2269\u001b[0m     ]\n\u001b[1;32m   2270\u001b[0m )\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mis_hyper_param_search \u001b[38;5;241m=\u001b[39m trial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/trainer.py:1158\u001b[0m, in \u001b[0;36mTrainer.create_optimizer_and_scheduler\u001b[0;34m(self, num_training_steps)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_optimizer_and_scheduler\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_training_steps: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;124;03m    Setup the optimizer and the learning rate scheduler.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    `create_scheduler`) in a subclass.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m IS_SAGEMAKER_MP_POST_1_10 \u001b[38;5;129;01mand\u001b[39;00m smp\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m   1160\u001b[0m         \u001b[38;5;66;03m# If smp >= 1.10 and fp16 is enabled, we unwrap the optimizer\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39moptimizer\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/trainer.py:1223\u001b[0m, in \u001b[0;36mTrainer.create_optimizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m optimizer_kwargs:\n\u001b[1;32m   1221\u001b[0m     optimizer_grouped_parameters \u001b[38;5;241m=\u001b[39m optimizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_grouped_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/optim/adamw.py:88\u001b[0m, in \u001b[0;36mAdamW.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     82\u001b[0m fused_supported_devices \u001b[38;5;241m=\u001b[39m _get_fused_kernels_supported_devices()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m     84\u001b[0m     p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(p)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     87\u001b[0m ):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fused=True` requires all the params to be floating point Tensors of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfused_supported_devices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreach:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fused` and `foreach` cannot be `True` together.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `fused=True` requires all the params to be floating point Tensors of supported devices: ['cuda', 'xpu', 'cpu', 'privateuseone']."
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning answerdotai/ModernBERT-base on ~15,000 synthetic prompts for 5 epochs took 321 seconds and our best model achieved a f1 score of 0.993. ğŸš€ I also ran the training with bert-base-uncased to compare the training time and performance. The original BERT achieved a f1 score of 0.99 and took 1048 seconds to train.  \n",
    "Note: ModernBERT and BERT both almost achieve the same performance. This indicates that the dataset is not challenging and probably could be solved using a logistic regression classifier. I ran the same code on the banking77 dataset. A dataset of ~13,000 customer service queries with 77 classes. There the ModernBERT outperformed the original BERT by 3% (f1 score of 0.93 vs 0.90)  \n",
    "Lets save our final best model and tokenizer to the Hugging Face Hub and create a model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processor and create model card\n",
    "tokenizer.save_pretrained(\"modernbert-llm-router\")\n",
    "trainer.create_model_card()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Inference & testÂ model\n",
    "To wrap up this tutorial, we will run inference on a few examples and test our model. We will use the pipeline method from the transformers library to run inference on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "# load model from huggingface.co/models using our repository id\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"modernbert-llm-router\", device=0)\n",
    " \n",
    "sample = \"How does the structure and function of plasmodesmata affect cell-to-cell communication and signaling in plant tissues, particularly in response to environmental stresses?\"\n",
    " \n",
    " \n",
    "pred = classifier(sample)\n",
    "print(pred)\n",
    "# [{'label': 'large_llm', 'score': 1.0}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
